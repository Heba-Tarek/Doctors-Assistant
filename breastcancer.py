# -*- coding: utf-8 -*-
"""BreastCancer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LghdzAGSE9W5IRc7P0KMl9z9DQFv0bIJ

# **Data Content**
1. Number of instances: 569 

2. Number of attributes: 32 (ID, diagnosis, 30 real-valued input features)

3. Attribute information

1) ID number
2) Diagnosis (M = malignant, B = benign)
3-32)

Ten real-valued features are computed for each cell nucleus:

	a) radius (mean of distances from center to points on the perimeter)
	b) texture (standard deviation of gray-scale values)
	c) perimeter
	d) area
	e) smoothness (local variation in radius lengths)
	f) compactness (perimeter^2 / area - 1.0)
	g) concavity (severity of concave portions of the contour)
	h) concave points (number of concave portions of the contour)
	i) symmetry 
	j) fractal dimension ("coastline approximation" - 1)

The mean, standard error, and "worst" or largest (mean of the three
largest values) of these features were computed for each image,
resulting in 30 features.  For instance, field 3 is Mean Radius, field
13 is Radius SE, field 23 is Worst Radius.

All feature values are recoded with four significant digits.

4. Missing attribute values: none

5. Class distribution: 357 benign, 212 malignant

# **Setup**
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
import warnings
warnings.filterwarnings('ignore')

from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC

from sklearn.feature_selection import RFECV
from sklearn.feature_selection import RFE 
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
from sklearn.model_selection import train_test_split

from sklearn.metrics import confusion_matrix, plot_confusion_matrix
from sklearn.metrics import plot_roc_curve, classification_report

from sklearn.datasets import load_breast_cancer

pd.pandas.set_option('display.max_columns', None)

"""# **Data Exploration**"""

df = pd.read_csv("dataset.csv")
df

df.shape

df.info()

df.describe().T

df.isnull().values.any()

B, M = df['diagnosis'].value_counts()
print('Number of Benign: ', B)
print('Number of Malignant : ', M)
print('Percentage of samples classified as Benign = ' + str((round((B/len(df['diagnosis'])), 3))*100) + '%')
print('Percentage of samples classified as Malignant = ' + str((round((M/len(df['diagnosis'])), 3))*100) + '%')

"""# **Data Cleaning**"""

df = df.drop(['id'], axis=1)

df['diagnosis']= df['diagnosis'].replace('B', 0)
df['diagnosis']= df['diagnosis'].replace('M', 1)

type(df['diagnosis'])

"""# **Data Visualization**"""

sns.countplot(df['diagnosis'], palette='RdBu')
plt.xticks([0, 1], ['Benign', 'Malignant'])
plt.xlabel('Tumor Classification')
plt.ylabel('Count')
plt.show()

"""## **Visualizing first 10 features:**"""

target = df['diagnosis']
data = df.drop('diagnosis', axis=1)
data_standardized = (data - data.mean()) / (data.std())
data = pd.concat([target, data_standardized.iloc[:,0:10]], axis=1)
data = pd.melt(data, id_vars="diagnosis", var_name="features", value_name='value')
plt.figure(figsize=(20,10))
plt.title("0: Benign     1: Malignant")
sns.violinplot(x="features", y="value", hue="diagnosis", data=data, split=True, inner="quart")
plt.xticks(rotation=90)

"""## **Visualizing second 10 features:**"""

data = pd.concat([target, data_standardized.iloc[:,10:20]], axis=1)
data = pd.melt(data, id_vars="diagnosis", var_name="features", value_name='value')
plt.figure(figsize=(20,10))
plt.title("0: Benign     1: Malignant")
sns.swarmplot(x="features", y="value", hue="diagnosis", data=data)
plt.xticks(rotation=90)

"""## **Visualizing third 10 features:**"""

data = pd.concat([target, data_standardized.iloc[:,20:30]], axis=1)
data = pd.melt(data, id_vars="diagnosis", var_name="features", value_name='value')
plt.figure(figsize=(20,10))
plt.title("0: Benign     1: Malignant")
sns.boxplot(x="features", y="value", hue="diagnosis", data=data)
plt.xticks(rotation=90)

"""# **Correlation Mapping and Analysis**"""

corr = df.corr()
plt.figure(figsize=(20,10))
sns.heatmap(corr, cmap="BuPu", annot=True)
plt.show()

corr_di = df.corr()['diagnosis']
print('Feature Correlation With Target:\n', corr_di)

feature_dict = dict(zip(df.columns, list(corr_di)))
feature_corr = pd.DataFrame(feature_dict, index=[0])
feature_corr.T.plot.bar(title="Feature Correlation With Target", legend=False, figsize=(20,10));

"""# **Preparing Data**"""

y = df['diagnosis']
X = df.drop('diagnosis', axis=1)

"""## Splitting data into training and testing sets"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

"""# **Modeling & Performance Analysis**

## Models vs. Accuracy: Recursive Feature Eliminition
"""

models1 = {"Logistic Regression": LogisticRegression(),
          "Decision Tree":DecisionTreeClassifier(),
          "XGBoost":XGBClassifier(),
          "Random Forest": RandomForestClassifier(random_state=43)}
    
model_scores1 = {}

for name1, model1 in models1.items(): 
    rfe1 = RFE(estimator=model1, n_features_to_select=8, step=1) 
    X_train1 = rfe1.fit_transform(X_train, y_train)
    X_test1 = rfe1.transform(X_test)
    model1.fit(X_train1, y_train)
    model_scores1[name1] = model1.score(X_test1, y_test)
    
evaluation1 = pd.DataFrame(model_scores1, index=["Accuracy"])
evaluation1

evaluation1.T.plot.bar(color='firebrick', grid='true')
plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))
plt.xlabel('Model')
plt.ylabel('Accuracy')

"""## Models vs. Accuracy: Recursive Feature Elimination with K-fold Crossvalidation"""

models2 = {"Logistic Regression": LogisticRegression(),
          "Decision Tree":DecisionTreeClassifier(),
          "XGBoost":XGBClassifier(),
          "Random Forest": RandomForestClassifier(random_state=43)}
    
model_scores2 = {}

for name2, model2 in models2.items(): 
    rfecv2 = RFECV(estimator=model2, step=1, cv=10, scoring='accuracy')
    X_train2 = rfecv2.fit_transform(X_train, y_train)
    X_test2 = rfecv2.transform(X_test)
    model2.fit(X_train2, y_train)
    model_scores2[name2] = model2.score(X_test2, y_test)
    
evaluation2 = pd.DataFrame(model_scores2, index=["Accuracy"])
evaluation2
#len(model2.feature_importances_)

"""## Models vs. Accuracy: Univariate Feature Selection - Select K-best"""

select_feature = SelectKBest(chi2, k=10).fit(X_train, y_train)
X_train3 = select_feature.transform(X_train)
X_test3 = select_feature.transform(X_test)

np.size(X_test3,1)

models3 = { "KNN": KNeighborsClassifier(),
          "Logistic Regression": LogisticRegression(),
          "Decision Tree":DecisionTreeClassifier(),
          "XGBoost":XGBClassifier(),
          "Random Forest": RandomForestClassifier(random_state=43),
          "SVC": SVC(),
          "Gaussian Naive Bayes":GaussianNB()}
    
model_scores3 = {}

for name3, model3 in models3.items():   
    model3.fit(X_train3, y_train)
    model_scores3[name3] = model3.score(X_test3, y_test)
    
evaluation3 = pd.DataFrame(model_scores3, index=["Accuracy"])
evaluation3

"""## Models Vs Accuracy: Feature Reduction - Principle Component Analysis (PCA)"""

np.size(X_test,1)

sc = StandardScaler()
 
X_train4 = sc.fit_transform(X_train)
X_test4 = sc.transform(X_test)
 
pca = PCA(n_components = 0.95)
 
X_train4 = pca.fit_transform(X_train4)
X_test4 = pca.transform(X_test4)

per_var = np.round(pca.explained_variance_ratio_*100, decimals=1)
labels = ['PC' + str(x) for x in range(1, len(per_var)+1)]

plt.figure(figsize=(20,10))
plt.bar(x=range(1,len(per_var)+1), height=per_var, tick_label=labels)
plt.ylabel('Percentage of Explained Variance')
plt.xlabel('Principle component')
plt.title('Scree PLot')
plt.show()

np.size(X_test4,1)

per_var

models4 = { "KNN": KNeighborsClassifier(),
          "Logistic Regression": LogisticRegression(),
          "Decision Tree":DecisionTreeClassifier(),
          "XGBoost":XGBClassifier(),
          "Random Forest": RandomForestClassifier(random_state=43),
          "SVC": SVC(),
          "Gaussian Naive Bayes":GaussianNB()}
    
model_scores4 = {}

for name4, model4 in models4.items():   
    model4.fit(X_train4, y_train)
    model_scores4[name4] = model4.score(X_test4, y_test)
    
evaluation4 = pd.DataFrame(model_scores4, index=["Accuracy"])
evaluation4

evaluation4.T.plot.bar(color='firebrick', grid='true')
plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))
plt.xlabel('Model')
plt.ylabel('Accuracy')

"""# **Hyperparameter tuning**

## Tuning KNN manually
"""

neighbors = np.arange(1, 21)
test_accuracy = np.empty(len(neighbors))
train_accuracy = np.empty(len(neighbors))

for i, k in enumerate(neighbors):
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train4, y_train)
    test_accuracy[i] = knn.score(X_test4, y_test)
    train_accuracy[i] = knn.score(X_train4, y_train)

plt.figure(figsize=(20,10))
plt.title('k-NN: Varying Number of Neighbors')
plt.plot(neighbors, test_accuracy, label = 'Testing Accuracy')
plt.plot(neighbors, train_accuracy, label = 'Training Accuracy')
plt.xlabel('Number of Neighbors (K)')
plt.ylabel('Accuracy')
plt.legend()
plt.show()
max_K = np.argmax(test_accuracy)+1        #index starts from 0 while K starts from 1 so to map right we add '1'
print("Maximum accuracy =", max(test_accuracy),"at K =", max_K)

"""# Tuning LogisticRegression with GridSearchCV"""

log_reg_grid = {"C": np.logspace(-4, 4, 30),
                "solver": ["liblinear"]}
gs_log_reg = GridSearchCV(LogisticRegression(),
                          param_grid=log_reg_grid,
                          cv=5,
                          verbose=True)
gs_log_reg.fit(X_train4, y_train);

#gs_log_reg.best_params_
gs_log_reg.best_estimator_

gs_log_reg.score(X_test4, y_test)

"""# **Final model with best peformance:**"""

model = RandomForestClassifier(random_state=43)
rfe = RFE(estimator=model, n_features_to_select=8, step=1) 
X_train_rfe = rfe.fit_transform(X_train, y_train)
X_test_rfe = rfe.transform(X_test)
model.fit(X_train_rfe, y_train)
y_test = y_test.to_numpy()
model.score(X_test_rfe, y_test)

filename = 'BreastCancer_model.sav'
pickle.dump(model, open(filename, 'wb'))

y_pred = model.predict(X_test_rfe)
print(classification_report(y_test, y_pred))
plot_roc_curve(model, X_test_rfe, y_test)

confusion_matrix(y_test, y_pred)

plot_confusion_matrix(model, X_test_rfe, y_test, cmap = "OrRd_r")
plt.grid(False)
plt.show()

X_train.columns[rfe.support_]

rfe.support_

X_train_rfe.dtype

input = [[1001, 0.3001, 0.1471, 25.38, 17.33, 184.6, 2019, 0.2654]]
prediction = model.predict(input)
print(prediction[0])

input = [[566.3, 0.06664, 0.04781, 15.11, 19.26, 99.7, 711.2, 0.1288]]
prediction = model.predict(input)
print(prediction[0])

input = X_test_rfe[[2]]
prediction = model.predict(input)
print(prediction[0])

y_test[2]

loaded_model = pickle.load(open(filename, 'rb'))
result = loaded_model.score(X_test_rfe, y_test)
print(result)

def predict_output(model, input):
  prediction = model.predict_proba(input)
  return prediction[0]

input0 = [[566.3, 0.06664, 0.04781, 15.11, 19.26, 99.7, 711.2, 0.1288]]
prediction = predict_output(loaded_model, input0)
print('Probability sample is benign: ' + str(prediction[0]*100) + '%')
print('Probability sample is Malignant: ' + str(prediction[1]*100) + '%')

input1 = [[1001, 0.3001, 0.1471, 25.38, 17.33, 184.6, 2019, 0.2654]]
prediction = predict_output(loaded_model, input1)
print('Probability sample is benign: ' + str(prediction[0]*100) + '%')
print('Probability sample is Malignant: ' + str(prediction[1]*100) + '%')

model.classes_

"""# New Section"""